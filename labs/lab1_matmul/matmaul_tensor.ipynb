{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bringing Tensor Cores to Standard Fortran\n",
    "===\n",
    "\n",
    "# Lab 1: MATMUL\n",
    "\n",
    "In this tutorial we will familiarize ourselves with how standard Fortran array intrinsic functions can be mapped to GPU-accelerated math libraries.\n",
    "\n",
    "At the simplest level, only two Fortran statements are required to take advantage of the outstanding performance provided by the cuTENSOR library.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <span style='background :lightgrey' > **use cutensorex** </span> \n",
    "\n",
    " <span style='background :lightgrey' > **.......**        </span> \n",
    "\n",
    "<span style='background :lightgrey' > **c = matmul(a,b)** </span> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A working implementation is provided in [main.f90](./main.f90) \n",
    "\n",
    "Please take 2-3 minutes to skim through it.\n",
    "\n",
    "## Getting started\n",
    "\n",
    "Let's start by checking the version of the NVIDIA Fortran compiler installed in the image:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nvfortran 23.3-0 64-bit target on x86-64 Linux -tp haswell \n",
      "NVIDIA Compilers and Tools\n",
      "Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n"
     ]
    }
   ],
   "source": [
    "!nvfortran --version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now let's compile and run (You may have a look at the Makefile for compiler options):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvfortran -acc -gpu=managed -cuda -cudalib main.f90\n",
      "/usr/bin/ld: cannot find -lcusolverMp: No such file or directory\n",
      "/usr/bin/ld: cannot find -lcal: No such file or directory\n",
      "/usr/bin/ld: cannot find -lcutensor: No such file or directory\n",
      "/usr/bin/ld: cannot find -lcutensorMg: No such file or directory\n",
      "/usr/bin/ld: cannot find -lnccl: No such file or directory\n",
      "/usr/bin/ld: cannot find -lnvshmem_device: No such file or directory\n",
      "/usr/bin/ld: cannot find -lnvshmem_host: No such file or directory\n",
      "pgacclnk: child process exit status 1: /usr/bin/ld\n",
      "make: *** [Makefile:3: all] Error 2\n",
      "/bin/bash: line 1: ./a.out: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!make\n",
    "!./a.out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below gives the performance of this code snippet on different architectures.\n",
    "\n",
    "Table 1. real(8) performance on one NUMA node of a dual-socket AMD EPYC 7742 Rome CPU-based server, a single V100, and a single A100 GPU.\n",
    "\n",
    "|Implementation / Processor\t| TFLOPs\n",
    "| :----: | :----: |\n",
    "|nvfortran matmul on a single CPU core | \t0.010\n",
    "|MKL DGEMM on 64 CPU cores |\t1.674\n",
    "|Naive OpenACC on V100|\t0.235\n",
    "|Naive OpenACC on A100 |\t0.447\n",
    "|nvfortran matmul on V100 |\t6.866\n",
    "|nvfortran matmul on A100 |\t17.660\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your performance is not in the same range for your architecture , compile with the Makefile in the solution folder : it is forcing a static link of the cuTENSOR library which avoids to load it repetitively in case of dynamic linking, which could be a costly operation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supported standard Fortran operations\n",
    "===\n",
    "\n",
    "The cuTENSOR library contains general permutation and contraction operations. The result of the permutation can optionally be operated on by an elemental function, and optionally scaled.\n",
    "\n",
    "The nvfortran compiler can recognize and map a variety of Fortran transformational intrinsics and elemental intrinsic functions used in combination with general array syntax to cuTENSOR functionality. A few of the more straightforward translations include the following:\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d = transpose(a)\n",
    "\n",
    "d = func(transpose(a))\n",
    "\n",
    "d = alpha * func(transpose(a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d = reshape(a,shape=[...])\n",
    "\n",
    "d = reshape(a,shape=[...],order=[...])\n",
    "\n",
    "d = func(reshape(a,...))\n",
    "\n",
    "d = alpha * func(reshape(a,...))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d = spread(a,dim=k,ncopies=n)\n",
    "\n",
    "d = func(spread(a,dim=k,ncopies=n))\n",
    "\n",
    "d = alpha * func(spread(a,dim=k,ncopies=n))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs to matmul() can also be permuted in cuTENSOR, and the result can be scaled and accumulated. That leads to several possible combinations, such as the following statements:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c = matmul(a,b)\n",
    "\n",
    "c = c + matmul(a,b)\n",
    "\n",
    "c = c - matmul(a,b)\n",
    "\n",
    "c = c + alpha * matmul(a,b)\n",
    "\n",
    "d = alpha * matmul(a,b) + beta * c\n",
    "\n",
    " \n",
    "c = matmul(transpose(a),b)\n",
    "\n",
    "c = matmul(reshape(a,shape=[...],order=[...]),b)\n",
    "\n",
    "c = matmul(a,transpose(b))\n",
    "\n",
    "c = matmul(a,reshape(b,shape=[...],order=[...]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
